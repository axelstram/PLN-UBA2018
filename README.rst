Informe

Como corpus se utilizo el libro "Hands-On Machine Learning with Scikit-Learn and TensorFlow" de Aurelien Geron 

-- Generación de modelos

A continuación se muestran secuencias de palabras generadas con los distintos modelos y sus perplexities asociadas.



Ngramas con n=1:

- of , , to of ) : ) ) the . ) , . to a , . ( the is , . in to the ) the to of and ) the , , of of and . . of ,


Ngramas con n=2:

- Figure 6-2 . , then you will get a bit of Machine Learning Project . , it to get the right , so the first . , but not to the model .

- For example , but this case , you get a bit . , which will have the data and so it will see Chapter 1 , but this book : from 0 ) : you are also create the same size , you want to use TensorFlow cluster . 0 ) .

- In a training data is called a model is a , but you want to a TensorFlow


Ngramas con n=3:

- This gives us the optimal number of instances with millions of parameters in the training data .
For example , if you are ready to train the system , called a deconvolutional layer , one per color chan nel .

- If your model , not the most common categories of ML systems : supervised versus unsupervised learning , the following code shows how to train the weights of the network to be pretty obvious that the classifier , but it will give it rewards any time it does not affect strat_train_set ) : X_batch , y : y_batch ) accuracy_score = accuracy .

- For each neuron can be reused fairly efficiently in any way to evaluate a node s gini attribute measures its impur ity : a View from the MNIST images ( e . g . , the output gate .


Ngramas con n=4:

- Chapter 13 : Convolutional Neural Networks
The resulting RNN is represented in 2D , which makes it a multioutput classifier .

- For each instance , and then we will pick action 0 with 70 probability , and action 1 with 30 probability .

- However , the TensorFlow team is working on improving the dynamic placer

- The hyperparameter controlling the regularization strength of a Scikit-Learn LogisticRegression model is not alpha ( as in Batch GD ) or based on just one bit ( 0 )



En estos casos la perplexity obtenida fue de 1.67, 1.23, 1.20, 1.25 respectivamente.





Addone con n=1:

- a that a and the , and , . . ) ) of , it , , to

Addone con n=2:

- If you may be used to be useful , and the right , the data . 0 . , so you may be an example , the inputs ( )

Addone con n=3:

- A good place to start the QueueRunner and tells it to 2D : import numpy as np > > from sklearn . ensemble import GradientBoostingRegressor gbrt = GradientBoostingRegressor ( max_dep

Addone con n=4:

- This will give us a 1D tensor containing a list of gradient vector variable pairs .



En estos casos la perplexity obtenida fue de 1.2041, 1.1967, 1.1965 y 1.2403
